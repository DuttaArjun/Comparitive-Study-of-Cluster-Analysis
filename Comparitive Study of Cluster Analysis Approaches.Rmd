---
title: "Comparitive Study of Cluster Analysis Approaches"
author: "Arjun Dutta"
date: "3 March 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Segmentation Philosophy

The general goal of market segmentation is to find groups of customers that differ in
important ways associated with product interest, market participation, or response
to marketing efforts.

# 2. What is Segmentation/Clustering?

Segmentation is like slicing a pie, and any pie might be sliced in an infinite number of ways.

We explore two distinct yet related areas of statistics:
Clustering or cluster analysis and Classification. 
These are the primary branches of what is sometimes called statistical learning.

## What is Supervised Learning ?

In supervised learning, a model is presented with observations whose outcome
status (dependent variable) is known, with a goal to predict that outcome from
the independent variables.

## What is Un-Supervised Learning ?

Un-Supervised learning we do not know the outcome groupings but attempt to
discover them from structure in the data.


## Load and Explore the Data

```{r cars}

seg.raw <- read.csv("http://goo.gl/qw303p")
seg.df <- seg.raw[ , -7] # remove the known segment assignments
summary(seg.df)

```

## Steps of Clustering 

Clustering analysis requires two stages: finding a proposed cluster solution and evaluating
that solution for one's business needs.

For each method we go through the following steps:

1. Transform the data if needed for a particular clustering method; for instance,
some methods require all numeric data (e.g., kmeans(), mclust()) or all
categorical data (e.g., poLCA()).

2. Compute a distance matrix if needed; some methods require a precomputed
matrix of similarity in order to group observations (e.g., hclust()) .

3. Apply the clustering method and save its result to an object. For some methods
this requires specifying the number (K) of groups desired (e.g., kmeans(),
poLCA()).

4. For some methods, further parse the object to obtain a solution with K groups
(e.g., hclust()).

5. Examine the solution in the model object with regard to the underlying data,
and consider whether it answers a business question.

```{r }
seg.summ <- function(data, groups) {aggregate(data, list(groups), function(x) mean(as.numeric(x)))}
```

## Heirarchial Clustering

Hierarchical clustering is a popular method that groups observations according to
their similarity.

hclust() is a distance-based algorithm that operates on a dissimilarity matrix, an
N-by-N matrix that reports a metric for the distance between each pair of observations.

## How it works

The hierarchical clustering method begins with each observation in its own cluster.
It then successively joins neighboring observations or clusters one at a time according
to their distances from one another, and continues this until all observations
are linked. This process of repeatedly joining observations and groups is known as
an agglomerative method.

There are many ways to compute distance, and we start by examining the
best-known method, the Euclidean distance.

The daisy() function in the cluster
package [108] works with mixed data types by rescaling the values, so we use that
instead of Euclidean distance:

```{r}
library(cluster) # daisy works with mixed data types
seg.dist <- daisy(seg.df)
seg.hc <- hclust(seg.dist, method="complete")

```

We use the complete linkage method, which evaluates the distance between every
member when combining observations and groups.

```{r}
plot(seg.hc)
plot(cut(as.dendrogram(seg.hc), h=0.5)$lower[[1]])
```

Finally, we might check one of the goodness-of-fit metrics for a hierarchical cluster
solution. One method is the cophenetic correlation coefficient (CPCC), which
assesses how well a dendrogram (in this case seg.hc) matches the true distance
metric (seg.dist) [145]. We use cophenetic() to get the distances from the
dendrogram, and compare it to the dist() metrics with cor():

```{r}
cor(cophenetic(seg.hc), seg.dist)
```

CPCC is interpreted similarly to Pearson's r. In this case, CPCC > 0.7 indicates
a relatively strong fit, meaning that the hierarchical tree represents the distances
between customers well.

## Kmeans

K-means clustering attempts to find groups that are most compact, in terms of the
mean sum-of-squares deviation of each observation from the multivariate center
(centroid) of its assigned group.

```{r}
seg.df.num <- seg.df
seg.df.num$gender <- ifelse(seg.df$gender=="Male", 0, 1)
seg.df.num$ownHome <- ifelse(seg.df$ownHome=="ownNo", 0, 1)
seg.df.num$subscribe <- ifelse(seg.df$subscribe=="subNo", 0, 1)
summary(seg.df.num)
```

```{r}
set.seed(9632)
seg.k <- kmeans(seg.df.num, centers=4)
boxplot(seg.df.num$income ~ seg.k$cluster, ylab="Income", xlab="Cluster")

```

```{r}
library(cluster)

clusplot(seg.df, seg.k$cluster, color=TRUE, shade=TRUE,labels=4, lines=0, main="K-means cluster plot")
```

## Model-Based Clustering: Mclust()

The key idea for model-based clustering is that observations come from groups
with different statistical distributions (such as different means and variances).

Such models are also known as "mixture models" because it is assumed that the
data reflect a mixture of observations drawn from different populations, although we
don't know which population each observation was drawn from.

As you might guess, because mclust models data with normal distributions, it uses
only numeric data.

```{r}
library(mclust)
seg.mc <- Mclust(seg.df.num)
summary(seg.mc)
```

This tells us that the data are estimated to have three clusters (components) with
the sizes as shown in the table. Mclust() compared a variety of different mixture
shapes and concluded that an ellipsoidal model (modeling the data as multivariate
ellipses) fit best.

Now, We try a 4-cluster solution by telling Mclust() the number of clusters we want with
the G=4 argument:

```{r}
seg.mc4 <- Mclust(seg.df.num, G=4)
summary(seg.mc4)
```

We compare the 3-cluster and 4-cluster models using the Bayesian information criterion
(BIC) [129] with BIC(model1, model2):

```{r}
BIC(seg.mc, seg.mc4)
```

The difference between the models is 181. The key point to interpreting BIC is to
remember this: the lower the value of BIC, on an infinite number line, the better. BIC
of ???1,000 is better than BIC of ???990; and BIC of 60 is better than BIC of 90.
There is one important note when interpreting BIC in R: unfortunately, some functions
return the negative of BIC, which would then have to be interpreted in the opposite
direction.We see above that BIC() reports positive values while Mclust()
returns the same values in the negative direction. If you are ever unsure of the direction
to interpret, use the BIC() function and interpret as noted (lower values are
better). Alternatively, you could also check the log-likelihood values, where higher
log-likelihood values are better (e.g., ???1,000 is better than ???1,100).

```{r}
seg.summ(seg.df, seg.mc$class)
clusplot(seg.df, seg.mc$class, color=TRUE, shade=TRUE, labels=4, lines=0, main="Model-based cluster plot")
```

## poLCA

Latent class analysis (LCA) is similar to mixture modeling in the assumption that
differences are attributable to unobserved groups that one wishes to uncover.

Whereas mclust and kmeans() work with numeric data, and hclust() depends
on the distance measure, poLCA uses only categorical variables.

There are several approaches to convert numeric data to factors, but for purposes
here we simply recode everything as binary with regard to a specified cutting point
(for instance, to recode as 1 for income below some cutoff and 2 above that). In the
present case, we split each variable at the median() and recode using ifelse()
and factor() (we'll see a more general approach to recoding numeric values with
cut()

```{r}
seg.df.cut <- seg.df
seg.df.cut$age <- factor(ifelse(seg.df$age < median(seg.df$age), 1, 2))
seg.df.cut$income <- factor(ifelse(seg.df$income < median(seg.df$income),1, 2))
seg.df.cut$kids <- factor(ifelse(seg.df$kids < median(seg.df$kids), 1, 2))
summary(seg.df.cut)
```